{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CSI 4142 Data Science** <br/>\n",
    "*Assignment 3: Predictive Analysis and Classification*\n",
    "\n",
    "# Identification\n",
    "\n",
    "Name: Eli Wynn<br/>\n",
    "Student Number: 300248135\n",
    "\n",
    "Name: Jack Snelgrove<br/>\n",
    "Student Number: 300247435\n",
    "\n",
    "\n",
    "Our datasets have been uploaded from the public repository:\n",
    "\n",
    "- [github.com/eli-wynn/Datasets](https://github.com/eli-wynn/Datasets)\n",
    "\n",
    "# Running Instructions\n",
    "1. Generate a kaggle Api key on the kaggle website under your Account < Settings < Generate Key\n",
    "2. This will generate a kaggle.json file\n",
    "3. Place the file in its respective location\n",
    "- On Windows: place the file here C:\\Users\\<YourUsername>\\.kaggle\\kaggle.json\n",
    "- On Mac: place the file here: ~/.kaggle/kaggle.json\n",
    "\n",
    "# Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#configure database -> can't use github because files are too large\n",
    "#!pip install kaggle\n",
    "#!pip install python-Levenshtein\n",
    "#Run the above lines if library has not been previously installed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import kaggle\n",
    "import Levenshtein\n",
    "import re\n",
    "import ast\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score, mean_squared_error, silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans, DBSCAN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "import os\n",
    "import kaggle\n",
    "\n",
    "# Set environment variables\n",
    "os.environ[\"KAGGLE_USERNAME\"] = \"eliwynn\"\n",
    "os.environ[\"KAGGLE_KEY\"] = \"c71d95504bba7ec81de8ed35ec02b166\"\n",
    "\n",
    "# Download the dataset\n",
    "dataset = \"rounakbanik/the-movies-dataset\"\n",
    "download_path = \"./movies_dataset\"\n",
    "\n",
    "#download entire dataset\n",
    "kaggle.api.dataset_download_files(dataset, download_path, unzip=True)\n",
    "\n",
    "metadataDF = pd.read_csv(f\"{download_path}/movies_metadata.csv\")\n",
    "metadataDF.head()\n",
    "ratingsDF = pd.read_csv(f\"{download_path}/ratings.csv\")\n",
    "ratingsDF.head()\n",
    "ratingsSmallDF = pd.read_csv(f\"{download_path}/ratings_small.csv\")\n",
    "ratingsSmallDF.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Dataset Description**\n",
    "\n",
    "## **Overview**\n",
    "| Attribute | Description |\n",
    "|-----------|-------------|\n",
    "| Dataset Name | The Movies Dataset |\n",
    "| Author | Rounak Banik |\n",
    "| Purpose | Provides metadata and ratings for movies to facilitate film analysis and recommendation system development |\n",
    "| Shape | 45,466 rows × 24 columns (movies metadata), 100,004 rows × 4 columns (ratings) |\n",
    "\n",
    "## **Features**\n",
    "\n",
    "### **Movies Metadata (`movies_metadata.csv`)**\n",
    "| Feature Name | Type | Description |\n",
    "|-------------|------|-------------|\n",
    "| id | Categorical | Unique identifier for each movie |\n",
    "| title | Categorical | Title of the movie |\n",
    "| release_date | DateTime | Release date of the movie |\n",
    "| budget | Numerical | Production budget in USD |\n",
    "| revenue | Numerical | Box office revenue in USD |\n",
    "| genres | Categorical | List of genres associated with the movie |\n",
    "| popularity | Numerical | Popularity score based on TMDb metrics |\n",
    "| vote_average | Numerical | Average user rating (0-10) |\n",
    "| vote_count | Numerical | Total number of votes received |\n",
    "| production_companies | Categorical | List of companies that produced the movie |\n",
    "| production_countries | Categorical | Countries where the movie was produced |\n",
    "| runtime | Numerical | Duration of the movie in minutes |\n",
    "| spoken_languages | Categorical | List of languages spoken in the movie |\n",
    "\n",
    "### **Ratings (`ratings_small.csv`)**\n",
    "| Feature Name | Type | Description |\n",
    "|-------------|------|-------------|\n",
    "| userId | Categorical | Unique identifier for each user |\n",
    "| movieId | Categorical | Unique identifier for each movie (links to `id` in movies metadata) |\n",
    "| rating | Numerical | User rating of the movie (0.5 - 5.0) |\n",
    "| timestamp | DateTime | Unix timestamp of when the rating was given |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_movie_datasets():\n",
    "    \"\"\"\n",
    "    Prepare the movie datasets (movies_metadata.csv and ratings_small.csv)\n",
    "    \"\"\"\n",
    "\n",
    "    metadataDF = pd.read_csv('./movies_dataset/movies_metadata.csv', low_memory=False)\n",
    "    ratingsDF = pd.read_csv('./movies_dataset/ratings_small.csv')\n",
    "    \n",
    "    # Display basic information\n",
    "    print(\"Movies Metadata Shape:\", metadataDF.shape)\n",
    "    print(\"Ratings Shape:\", ratingsDF.shape)\n",
    "    \n",
    "    # Clean the movies dataset\n",
    "    print(\"Cleaning movies dataset...\")\n",
    "    \n",
    "    # Convert 'id' to numeric, coercing errors to NaN\n",
    "    metadataDF['id'] = pd.to_numeric(metadataDF['id'], errors='coerce')\n",
    "    \n",
    "    # Drop rows with NaN in 'id'\n",
    "    metadataDF = metadataDF.dropna(subset=['id'])\n",
    "    \n",
    "    # Convert 'id' to integer\n",
    "    metadataDF['id'] = metadataDF['id'].astype(int)\n",
    "    \n",
    "    # Select relevant columns\n",
    "    metadataDF = metadataDF[['id', 'title', 'release_date', 'popularity', 'vote_average', 'vote_count', 'budget', 'revenue', 'runtime', 'genres', 'overview']]\n",
    "    \n",
    "    # Convert date column to datetime\n",
    "    metadataDF['release_date'] = pd.to_datetime(metadataDF['release_date'], errors='coerce')\n",
    "    \n",
    "    # Extract year from release_date\n",
    "    metadataDF['release_year'] = metadataDF['release_date'].dt.year\n",
    "    \n",
    "    # Drop rows with missing values in important columns\n",
    "    metadataDF = metadataDF.dropna(subset=['release_year', 'popularity', 'vote_average', 'vote_count'])\n",
    "    \n",
    "    # Convert numeric columns to appropriate types\n",
    "    numeric_cols = ['popularity', 'vote_average', 'vote_count', 'budget', 'revenue', 'runtime']\n",
    "    for col in numeric_cols:\n",
    "        metadataDF[col] = pd.to_numeric(metadataDF[col], errors='coerce')\n",
    "    \n",
    "    # Fill missing values with median\n",
    "    for col in numeric_cols:\n",
    "        metadataDF[col] = metadataDF[col].fillna(metadataDF[col].median())\n",
    "    \n",
    "    # Prepare the ratings dataset\n",
    "    print(\"Preparing ratings dataset...\")\n",
    "    \n",
    "    # Rename movieId to match with metadataDF id\n",
    "    ratingsDF = ratingsDF.rename(columns={'movieId': 'id'})\n",
    "    \n",
    "    # Merge datasets\n",
    "    print(\"Merging datasets...\")\n",
    "    merged_df = pd.merge(ratingsDF, metadataDF, on='id', how='inner')\n",
    "    \n",
    "    print(\"Merged dataset shape:\", merged_df.shape)\n",
    "    \n",
    "    # Create features and target\n",
    "    # Target: High rating (1 if rating >= 4.0, 0 otherwise)\n",
    "    merged_df['high_rating'] = (merged_df['rating'] >= 4.0).astype(int)\n",
    "    \n",
    "    # Features: movie attributes\n",
    "    features = ['popularity', 'vote_average', 'vote_count', 'budget', 'revenue', 'runtime', 'release_year']\n",
    "    X = merged_df[features]\n",
    "    y = merged_df['high_rating']\n",
    "    \n",
    "    # Split into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    print(\"Movie datasets prepared successfully!\")\n",
    "    return X_train_scaled, X_test_scaled, y_train, y_test, merged_df, metadataDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Study 1: Similarity Measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test, merged_df, metadataDF = prepare_movie_datasets()\n",
    "\n",
    "# Helper functions for processing movie data\n",
    "def extract_genres(genres_json):\n",
    "    \"\"\"Extract genre names from the JSON string in the genres column\"\"\"\n",
    "    try:\n",
    "        if isinstance(genres_json, str):\n",
    "            genres = ast.literal_eval(genres_json)\n",
    "            return [genre['name'] for genre in genres]\n",
    "        else:\n",
    "            return []\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "def clean_title(title):\n",
    "    \"\"\"Clean title for better similarity matching\"\"\"\n",
    "    if pd.isna(title):\n",
    "        return \"\"\n",
    "    # Remove special characters and convert to lowercase\n",
    "    return re.sub(r'[^\\w\\s]', '', title).lower()\n",
    "\n",
    "def extract_keywords(overview):\n",
    "    \"\"\"Extract keywords from overview text\"\"\"\n",
    "    if pd.isna(overview):\n",
    "        return \"\"\n",
    "    # Simple keyword extraction - remove stopwords and keep only words with 4+ characters\n",
    "    words = re.findall(r'\\b\\w{4,}\\b', overview.lower())\n",
    "    # Remove common stopwords\n",
    "    stopwords = ['this', 'that', 'with', 'from', 'have', 'they', 'will', 'what', 'when', 'where', 'which']\n",
    "    return ' '.join([w for w in words if w not in stopwords])\n",
    "\n",
    "# Prepare additional features for similarity measures\n",
    "print(\"Preparing data for similarity measures...\")\n",
    "metadataDF['genres_list'] = metadataDF['genres'].apply(extract_genres)\n",
    "metadataDF['clean_title'] = metadataDF['title'].apply(clean_title)\n",
    "metadataDF['keywords'] = metadataDF['overview'].apply(extract_keywords)\n",
    "\n",
    "# 1. Jaccard Similarity for Genres\n",
    "def jaccard_similarity_genres(movie_data, movie_title):\n",
    "    \"\"\"\n",
    "    Jaccard similarity for genres\n",
    "    Jaccard(A,B) = |A ∩ B| / |A ∪ B|\n",
    "    \"\"\"\n",
    "    # Find the movie by title\n",
    "    movie = movie_data[movie_data['title'] == movie_title]\n",
    "    if len(movie) == 0:\n",
    "        print(f\"Movie '{movie_title}' not found. Please check the title.\")\n",
    "        return []\n",
    "    \n",
    "    movie = movie.iloc[0]\n",
    "    movie_genres = set(movie['genres_list'])\n",
    "    \n",
    "    # Calculate Jaccard similarity for all movies\n",
    "    similarities = []\n",
    "    for idx, row in movie_data.iterrows():\n",
    "        other_genres = set(row['genres_list'])\n",
    "        if not movie_genres or not other_genres:\n",
    "            sim = 0\n",
    "        else:\n",
    "            intersection = len(movie_genres.intersection(other_genres))\n",
    "            union = len(movie_genres.union(other_genres))\n",
    "            sim = intersection / union if union > 0 else 0\n",
    "        similarities.append((row['title'], sim, row['genres_list'], row['popularity']))\n",
    "    \n",
    "    # Sort by similarity (descending) and then by popularity (descending)\n",
    "    return sorted(similarities, key=lambda x: (-x[1], -x[3]))\n",
    "\n",
    "# 2. Euclidean Similarity for Revenue\n",
    "def euclidean_similarity_revenue(movie_data, movie_title):\n",
    "    \"\"\"\n",
    "    Euclidean distance for revenue\n",
    "    Converted to similarity: 1 / (1 + distance)\n",
    "    \"\"\"\n",
    "    # Find the movie by title\n",
    "    movie = movie_data[movie_data['title'] == movie_title]\n",
    "    if len(movie) == 0:\n",
    "        print(f\"Movie '{movie_title}' not found. Please check the title.\")\n",
    "        return []\n",
    "    \n",
    "    movie = movie.iloc[0]\n",
    "    movie_revenue = movie['revenue']\n",
    "    \n",
    "    # Calculate Euclidean similarity for all movies\n",
    "    similarities = []\n",
    "    for idx, row in movie_data.iterrows():\n",
    "        other_revenue = row['revenue']\n",
    "        # Calculate Euclidean distance\n",
    "        distance = abs(movie_revenue - other_revenue)\n",
    "        # Convert to similarity (higher is more similar)\n",
    "        sim = 1 / (1 + distance/1e6)  # Normalize by dividing by 1 million\n",
    "        similarities.append((row['title'], sim, row['revenue'], row['popularity']))\n",
    "    \n",
    "    # Sort by similarity (descending) and then by popularity (descending)\n",
    "    return sorted(similarities, key=lambda x: (-x[1], -x[3]))\n",
    "\n",
    "# 3. Manhattan Similarity for Runtime\n",
    "def manhattan_similarity_runtime(movie_data, movie_title):\n",
    "    \"\"\"\n",
    "    Manhattan distance for runtime\n",
    "    Converted to similarity: 1 / (1 + distance)\n",
    "    \"\"\"\n",
    "    # Find the movie by title\n",
    "    movie = movie_data[movie_data['title'] == movie_title]\n",
    "    if len(movie) == 0:\n",
    "        print(f\"Movie '{movie_title}' not found. Please check the title.\")\n",
    "        return []\n",
    "    \n",
    "    movie = movie.iloc[0]\n",
    "    movie_runtime = movie['runtime']\n",
    "    \n",
    "    # Calculate Manhattan similarity for all movies\n",
    "    similarities = []\n",
    "    for idx, row in movie_data.iterrows():\n",
    "        other_runtime = row['runtime']\n",
    "        # Calculate Manhattan distance\n",
    "        distance = abs(movie_runtime - other_runtime)\n",
    "        # Convert to similarity (higher is more similar)\n",
    "        sim = 1 / (1 + distance/10)  # Normalize by dividing by 10\n",
    "        similarities.append((row['title'], sim, row['runtime'], row['popularity']))\n",
    "    \n",
    "    # Sort by similarity (descending) and then by popularity (descending)\n",
    "    return sorted(similarities, key=lambda x: (-x[1], -x[3]))\n",
    "\n",
    "# 4. Levenshtein (Edit Distance) Similarity for Title\n",
    "def levenshtein_similarity_title(movie_data, movie_title):\n",
    "    \"\"\"\n",
    "    Levenshtein (edit) distance for titles\n",
    "    Converted to similarity: 1 - (distance / max_length)\n",
    "    \"\"\"\n",
    "    # Find the movie by title\n",
    "    movie = movie_data[movie_data['title'] == movie_title]\n",
    "    if len(movie) == 0:\n",
    "        print(f\"Movie '{movie_title}' not found. Please check the title.\")\n",
    "        return []\n",
    "    \n",
    "    movie = movie.iloc[0]\n",
    "    movie_clean_title = movie['clean_title']\n",
    "    \n",
    "    # Calculate Levenshtein similarity for all movies\n",
    "    similarities = []\n",
    "    for idx, row in movie_data.iterrows():\n",
    "        other_clean_title = row['clean_title']\n",
    "        # Calculate Levenshtein distance\n",
    "        distance = Levenshtein.distance(movie_clean_title, other_clean_title)\n",
    "        # Convert to similarity (higher is more similar)\n",
    "        max_len = max(len(movie_clean_title), len(other_clean_title))\n",
    "        sim = 1 - (distance / max_len) if max_len > 0 else 0\n",
    "        similarities.append((row['title'], sim, distance, row['popularity']))\n",
    "    \n",
    "    # Sort by similarity (descending) and then by popularity (descending)\n",
    "    return sorted(similarities, key=lambda x: (-x[1], -x[3]))\n",
    "\n",
    "# 5. Cosine Similarity for Budget\n",
    "def cosine_similarity_budget(movie_data, movie_title):\n",
    "    \"\"\"\n",
    "    Cosine similarity for budget\n",
    "    \"\"\"\n",
    "    # Find the movie by title\n",
    "    movie = movie_data[movie_data['title'] == movie_title]\n",
    "    if len(movie) == 0:\n",
    "        print(f\"Movie '{movie_title}' not found. Please check the title.\")\n",
    "        return []\n",
    "    \n",
    "    movie = movie.iloc[0]\n",
    "    movie_budget = movie['budget']\n",
    "    \n",
    "    # Calculate Cosine similarity for all movies\n",
    "    similarities = []\n",
    "    for idx, row in movie_data.iterrows():\n",
    "        other_budget = row['budget']\n",
    "        # Calculate Cosine similarity\n",
    "        if movie_budget == 0 and other_budget == 0:\n",
    "            sim = 1  # Both budgets are 0, consider them similar\n",
    "        elif movie_budget == 0 or other_budget == 0:\n",
    "            sim = 0  # One budget is 0, the other isn't\n",
    "        else:\n",
    "            # Cosine similarity for 1D is just the dot product divided by magnitudes\n",
    "            sim = (movie_budget * other_budget) / (abs(movie_budget) * abs(other_budget))\n",
    "        similarities.append((row['title'], sim, row['budget'], row['popularity']))\n",
    "    \n",
    "    # Sort by similarity (descending) and then by popularity (descending)\n",
    "    return sorted(similarities, key=lambda x: (-x[1], -x[3]))\n",
    "\n",
    "def display_results(results, title, attribute_name, top_n=10):\n",
    "    \"\"\"Display the top N results in a formatted way\"\"\"\n",
    "    print(f\"\\n--- Top {top_n} movies with similar {attribute_name} to '{title}' ---\")\n",
    "    \n",
    "    # Create a DataFrame for better display in notebook\n",
    "    result_df = pd.DataFrame([\n",
    "        {\n",
    "            'Title': movie_title,\n",
    "            'Similarity': f\"{similarity:.4f}\",\n",
    "            f'{attribute_name.capitalize()}': attribute,\n",
    "            'Popularity': f\"{popularity:.2f}\"\n",
    "        }\n",
    "        for movie_title, similarity, attribute, popularity in results[:top_n+1]\n",
    "        if movie_title != title  # Skip the query movie itself\n",
    "    ][:top_n])\n",
    "    \n",
    "    display(result_df)\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "# Run the similarity study\n",
    "print(\"\\n=== Study 1 – Similarity Measures ===\")\n",
    "\n",
    "# Define query movies\n",
    "toy_story = \"Toy Story\"\n",
    "titanic = \"Titanic\"\n",
    "apollo_13 = \"Apollo 13\"\n",
    "fight_club = \"Fight Club\"\n",
    "matrix = \"The Matrix\"\n",
    "\n",
    "# Verify movies exist in the dataset\n",
    "for movie in [toy_story, titanic, apollo_13, fight_club, matrix]:\n",
    "    if movie not in metadataDF['title'].values:\n",
    "        print(f\"Warning: '{movie}' not found in dataset. Using a similar title.\")\n",
    "        # Find closest match\n",
    "        closest = metadataDF.iloc[metadataDF['title'].apply(\n",
    "            lambda x: Levenshtein.distance(str(x).lower(), movie.lower())).argmin()]['title']\n",
    "        print(f\"Using '{closest}' instead of '{movie}'\")\n",
    "        if movie == toy_story:\n",
    "            toy_story = closest\n",
    "        elif movie == titanic:\n",
    "            titanic = closest\n",
    "        elif movie == apollo_13:\n",
    "            apollo_13 = closest\n",
    "        elif movie == fight_club:\n",
    "            fight_club = closest\n",
    "        elif movie == matrix:\n",
    "            matrix = closest\n",
    "\n",
    "# 1. Jaccard similarity for genres\n",
    "print(\"\\nRequest 1: Show me movies of the same genre as 'Toy Story'\")\n",
    "genre_results = jaccard_similarity_genres(metadataDF, toy_story)\n",
    "genre_df = display_results(genre_results, toy_story, \"genres\")\n",
    "\n",
    "# 2. Euclidean similarity for revenue\n",
    "print(\"\\nRequest 2: Show me movies with similar revenue to 'Titanic'\")\n",
    "revenue_results = euclidean_similarity_revenue(metadataDF, titanic)\n",
    "revenue_df = display_results(revenue_results, titanic, \"revenue\")\n",
    "\n",
    "# 3. Manhattan similarity for runtime\n",
    "print(\"\\nRequest 3: Show me movies with similar length as 'Apollo 13'\")\n",
    "runtime_results = manhattan_similarity_runtime(metadataDF, apollo_13)\n",
    "runtime_df = display_results(runtime_results, apollo_13, \"runtime (minutes)\")\n",
    "\n",
    "# 4. Levenshtein similarity for title\n",
    "print(\"\\nRequest 4: Show me movies with similar title to 'Fight Club'\")\n",
    "title_results = levenshtein_similarity_title(metadataDF, fight_club)\n",
    "title_df = display_results(title_results, fight_club, \"title\")\n",
    "\n",
    "# 5. Cosine similarity for budget\n",
    "print(\"\\nRequest 5: Show me movies with similar budget to 'The Matrix'\")\n",
    "budget_results = cosine_similarity_budget(metadataDF, matrix)\n",
    "budget_df = display_results(budget_results, matrix, \"budget\")\n",
    "\n",
    "# Store all results for further analysis\n",
    "similarity_results = {\n",
    "    'genre': genre_df,\n",
    "    'revenue': revenue_df,\n",
    "    'runtime': runtime_df,\n",
    "    'title': title_df,\n",
    "    'budget': budget_df\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Study 2: Clustering Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform KMeans clustering\n",
    "def perform_kmeans(data, feature_pairs, k_values):\n",
    "    results = {}\n",
    "    \n",
    "    for features in feature_pairs:\n",
    "        feature_name = f\"{features[0]}_vs_{features[1]}\"\n",
    "        results[feature_name] = {}\n",
    "        \n",
    "        # Extract features\n",
    "        X = data[list(features)].values\n",
    "        \n",
    "        # Standardize features\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X)\n",
    "        \n",
    "        for k in k_values:\n",
    "            # Perform KMeans clustering\n",
    "            kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "            clusters = kmeans.fit_predict(X_scaled)\n",
    "            \n",
    "            # Calculate silhouette score if k > 1\n",
    "            if k > 1:\n",
    "                silhouette = silhouette_score(X_scaled, clusters)\n",
    "            else:\n",
    "                silhouette = 0  # Not applicable for k=1\n",
    "            \n",
    "            # Store results\n",
    "            results[feature_name][k] = {\n",
    "                'clusters': clusters,\n",
    "                'centers': scaler.inverse_transform(kmeans.cluster_centers_),\n",
    "                'inertia': kmeans.inertia_,\n",
    "                'silhouette': silhouette\n",
    "            }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Function to perform DBSCAN clustering\n",
    "def perform_dbscan(data, feature_pairs, eps_values, min_samples_values):\n",
    "    results = {}\n",
    "    \n",
    "    for features in feature_pairs:\n",
    "        feature_name = f\"{features[0]}_vs_{features[1]}\"\n",
    "        results[feature_name] = {}\n",
    "        \n",
    "        # Extract features\n",
    "        X = data[list(features)].values\n",
    "        \n",
    "        # Standardize features\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X)\n",
    "        \n",
    "        for eps in eps_values:\n",
    "            for min_samples in min_samples_values:\n",
    "                param_key = f\"eps_{eps}_min_samples_{min_samples}\"\n",
    "                \n",
    "                # Perform DBSCAN clustering\n",
    "                dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "                clusters = dbscan.fit_predict(X_scaled)\n",
    "                \n",
    "                # Calculate silhouette score if more than one cluster and no noise points (-1)\n",
    "                unique_clusters = np.unique(clusters)\n",
    "                if len(unique_clusters) > 1 and -1 not in unique_clusters:\n",
    "                    silhouette = silhouette_score(X_scaled, clusters)\n",
    "                else:\n",
    "                    silhouette = 0  # Not applicable\n",
    "                \n",
    "                # Store results\n",
    "                results[feature_name][param_key] = {\n",
    "                    'clusters': clusters,\n",
    "                    'n_clusters': len(np.unique(clusters[clusters >= 0])),\n",
    "                    'n_noise': np.sum(clusters == -1),\n",
    "                    'silhouette': silhouette\n",
    "                }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Function to visualize clustering results\n",
    "def visualize_clustering(data, feature_pairs, kmeans_results, dbscan_results, k_values, eps_values, min_samples_values):\n",
    "    for features in feature_pairs:\n",
    "        feature_name = f\"{features[0]}_vs_{features[1]}\"\n",
    "        x_label = features[0]\n",
    "        y_label = features[1]\n",
    "        \n",
    "        # Create figure for this feature pair\n",
    "        fig = plt.figure(figsize=(20, 10))\n",
    "        \n",
    "        # Plot KMeans results\n",
    "        for i, k in enumerate(k_values):\n",
    "            ax = fig.add_subplot(2, len(k_values) + len(eps_values) * len(min_samples_values), i + 1)\n",
    "            \n",
    "            # Get cluster assignments\n",
    "            clusters = kmeans_results[feature_name][k]['clusters']\n",
    "            centers = kmeans_results[feature_name][k]['centers']\n",
    "            \n",
    "            # Plot data points colored by cluster\n",
    "            scatter = ax.scatter(data[x_label], data[y_label], c=clusters, cmap='viridis', \n",
    "                       alpha=0.5, s=10)\n",
    "            \n",
    "            # Plot cluster centers\n",
    "            ax.scatter(centers[:, 0], centers[:, 1], c='red', marker='X', s=100, label='Centroids')\n",
    "            \n",
    "            ax.set_title(f'KMeans (k={k})\\nInertia: {kmeans_results[feature_name][k][\"inertia\"]:.2f}\\nSilhouette: {kmeans_results[feature_name][k][\"silhouette\"]:.2f}')\n",
    "            ax.set_xlabel(x_label)\n",
    "            ax.set_ylabel(y_label)\n",
    "            ax.legend()\n",
    "            \n",
    "            # Add colorbar\n",
    "            plt.colorbar(scatter, ax=ax, label='Cluster')\n",
    "        \n",
    "        # Plot DBSCAN results\n",
    "        plot_idx = len(k_values) + 1\n",
    "        for eps in eps_values:\n",
    "            for min_samples in min_samples_values:\n",
    "                param_key = f\"eps_{eps}_min_samples_{min_samples}\"\n",
    "                \n",
    "                ax = fig.add_subplot(2, len(k_values) + len(eps_values) * len(min_samples_values), plot_idx)\n",
    "                \n",
    "                # Get cluster assignments\n",
    "                clusters = dbscan_results[feature_name][param_key]['clusters']\n",
    "                \n",
    "                # Plot data points colored by cluster\n",
    "                scatter = ax.scatter(data[x_label], data[y_label], c=clusters, cmap='viridis', \n",
    "                           alpha=0.5, s=10)\n",
    "                \n",
    "                ax.set_title(f'DBSCAN (eps={eps}, min_samples={min_samples})\\nClusters: {dbscan_results[feature_name][param_key][\"n_clusters\"]}\\nNoise: {dbscan_results[feature_name][param_key][\"n_noise\"]}\\nSilhouette: {dbscan_results[feature_name][param_key][\"silhouette\"]:.2f}')\n",
    "                ax.set_xlabel(x_label)\n",
    "                ax.set_ylabel(y_label)\n",
    "                \n",
    "                # Add colorbar\n",
    "                plt.colorbar(scatter, ax=ax, label='Cluster')\n",
    "                \n",
    "                plot_idx += 1\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'clustering_{feature_name}.png', dpi=300)\n",
    "        plt.show()\n",
    "\n",
    "# Main function to run the clustering study\n",
    "def run_clustering_study(metadataDF):\n",
    "    movie_data = metadataDF.copy()\n",
    "    movie_data = movie_data[(movie_data['budget'] > 0) & (movie_data['revenue'] > 0)]\n",
    "    \n",
    "    # Log transform budget and revenue (they're highly skewed)\n",
    "    movie_data['log_budget'] = np.log1p(metadataDF['budget'])\n",
    "    movie_data['log_revenue'] = np.log1p(movie_data['revenue'])\n",
    "    print(f\"Loaded {len(movie_data)} movies.\")\n",
    "    \n",
    "    # Define feature pairs to analyze\n",
    "    feature_pairs = [\n",
    "        ('log_budget', 'log_revenue'),  # Budget vs Revenue\n",
    "        ('runtime', 'log_budget')       # Runtime vs Budget\n",
    "    ]\n",
    "    \n",
    "    # Define parameters to test\n",
    "    k_values = [3, 5]                   # Number of clusters for KMeans\n",
    "    eps_values = [0.5, 1.0]             # Epsilon values for DBSCAN\n",
    "    min_samples_values = [5, 10]        # Minimum samples for DBSCAN\n",
    "    \n",
    "    # Perform clustering\n",
    "    print(\"\\nPerforming KMeans clustering...\")\n",
    "    kmeans_results = perform_kmeans(movie_data, feature_pairs, k_values)\n",
    "    \n",
    "    print(\"\\nPerforming DBSCAN clustering...\")\n",
    "    dbscan_results = perform_dbscan(movie_data, feature_pairs, eps_values, min_samples_values)\n",
    "    \n",
    "    # Visualize results\n",
    "    print(\"\\nVisualizing clustering results...\")\n",
    "    visualize_clustering(movie_data, feature_pairs, kmeans_results, dbscan_results, \n",
    "                        k_values, eps_values, min_samples_values)\n",
    "    \n",
    "    print(\"\\nClustering study completed!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_clustering_study(metadataDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The visualizations suggest that a hybrid approach might be optimal - using DBSCAN for initial outlier detection and pattern discovery, followed by KMeans on the core data points for more structured segmentation. DBSCAN seems to work better for identifying natural clusters in the log budget vs log revenue relationship because there is a strong positive correlation between the two variables. KMeans seems more effective for the runtime vs log revenue relationship because there is a weaker positive correlation and there are fewer clusters so forcing all of the points together would work better.\n",
    "\n",
    "## Study 3: Content-Based Recommendation System\n",
    "\n",
    "### Heuristic 1: Combines Jaccard similarity on genres and Euclidean similarity on popularity.\n",
    "\n",
    "Jaccard Similarity (Genres): This heuristic is based on the fact that movies with similar genres are likely to be similar in terms of user preferences. For instance, action movie fans might also enjoy other action movies.\n",
    "\n",
    "Euclidean Similarity (Popularity): This approach assumes that the popularity of a movie is correlated with its appeal. More popular movies could attract users with similar interests.\n",
    "\n",
    "### Heuristic 2: Combines Levenshtein (edit) distance on titles and Cosine similarity on budget.\n",
    "\n",
    "Levenshtein Similarity (Titles): Title similarity is used to match movies with similar titles (e.g., sequels or movies from the same series). This helps identify movies with names that are alike, which might be conceptually linked in users' minds.\n",
    "\n",
    "Cosine Similarity (Budget): Budget reflects production quality, which might influence the viewer's preference. Movies with similar budgets could be similar in terms of production values, targeting the same type of audience\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_heuristic_results(heuristic_results, title, top_n=10):\n",
    "    \"\"\"\n",
    "    Display the top N results for a heuristic in a formatted table.\n",
    "    Args:\n",
    "        heuristic_results (list): List of tuples containing (movie_title, combined_similarity, popularity).\n",
    "        title (str): The movie title that the user is querying for.\n",
    "        top_n (int): Number of top results to display.\n",
    "    \"\"\"\n",
    "    # Create a DataFrame for the heuristic results with only Title, Similarity, and Popularity\n",
    "    result_df = pd.DataFrame([\n",
    "        {\n",
    "            'Title': movie_title,\n",
    "            'Similarity': f\"{combined_similarity:.4f}\",\n",
    "            'Popularity': f\"{popularity:.2f}\"\n",
    "        }\n",
    "        for movie_title, combined_similarity, popularity in heuristic_results[:top_n+1]\n",
    "        if movie_title != title  # Skip the query movie itself\n",
    "    ][:top_n])\n",
    "    \n",
    "    # Sort by combined similarity (descending) and popularity (descending)\n",
    "    result_df_sorted = result_df.sort_values(by=['Similarity', 'Popularity'], ascending=[False, False])\n",
    "    \n",
    "    # Display the DataFrame (in a Jupyter notebook, this will show the table)\n",
    "    display(result_df_sorted)\n",
    "    \n",
    "    return result_df_sorted\n",
    "\n",
    "\n",
    "# Combine Jaccard on genres and Euclidean distance on popularity\n",
    "def combined_heuristic_1(movie_data, movie_title):\n",
    "    genre_results = jaccard_similarity_genres(movie_data, movie_title)\n",
    "    popularity_results = euclidean_similarity_revenue(movie_data, movie_title)\n",
    "    \n",
    "    combined_results = []\n",
    "    \n",
    "    # Loop through the results and combine the similarity scores\n",
    "    for genre_result, popularity_result in zip(genre_results, popularity_results):\n",
    "        combined_similarity = genre_result[1] * 0.6 + popularity_result[1] * 0.4  # The combined similarity score\n",
    "        movie_title = genre_result[0]  # Movie title\n",
    "        popularity = genre_result[3]  # Popularity from genre_results\n",
    "        \n",
    "        # Append only the relevant data: Title, Combined Similarity, and Popularity\n",
    "        combined_results.append((\n",
    "            movie_title, combined_similarity, popularity\n",
    "        ))\n",
    "    \n",
    "    # Sort by combined similarity (descending) and then by popularity (descending)\n",
    "    return sorted(combined_results, key=lambda x: (-x[1], -x[2]))\n",
    "\n",
    "# Combine Levenshtein similarity on titles and Cosine similarity on budget\n",
    "def combined_heuristic_2(movie_data, movie_title):\n",
    "    title_results = levenshtein_similarity_title(movie_data, movie_title)\n",
    "    budget_results = cosine_similarity_budget(movie_data, movie_title)\n",
    "    \n",
    "    combined_results = []\n",
    "    \n",
    "    # Loop through the results and combine the similarity scores\n",
    "    for title_result, budget_result in zip(title_results, budget_results):\n",
    "        combined_similarity = title_result[1] * 0.5 + budget_result[1] * 0.5  # The combined similarity score\n",
    "        movie_title = title_result[0]  # Movie title\n",
    "        popularity = title_result[3]  # Popularity from title_results\n",
    "        \n",
    "        # Append only the relevant data: Title, Combined Similarity, and Popularity\n",
    "        combined_results.append((\n",
    "            movie_title, combined_similarity, popularity\n",
    "        ))\n",
    "    \n",
    "    # Sort by combined similarity (descending) and then by popularity (descending)\n",
    "    return sorted(combined_results, key=lambda x: (-x[1], -x[2]))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Test combined heuristic 1 with 3 requests\n",
    "\n",
    "print(\"\\nRequest 1: Show me movies similar to 'Toy Story' using Heuristic 1\")\n",
    "heuristic_1_results_toy_story = combined_heuristic_1(metadataDF, \"Toy Story\")\n",
    "heuristic_1_df_toy_story = display_heuristic_results(heuristic_1_results_toy_story, \"Toy Story\", top_n=10)\n",
    "\n",
    "print(\"\\nRequest 2: Show me movies similar to 'Apollo 13' using Heuristic 1\")\n",
    "heuristic_1_results_apollo_13 = combined_heuristic_1(metadataDF, \"Apollo 13\")\n",
    "heuristic_1_df_apollo_13 = display_heuristic_results(heuristic_1_results_apollo_13, \"Apollo 13\", top_n=10)\n",
    "\n",
    "print(\"\\nRequest 3: Show me movies similar to 'Fight Club' using Heuristic 1\")\n",
    "heuristic_1_results_fight_club = combined_heuristic_1(metadataDF, \"Fight Club\")\n",
    "heuristic_1_df_fight_club = display_heuristic_results(heuristic_1_results_fight_club, \"Fight Club\", top_n=10)\n",
    "\n",
    "# Test combined heuristic 2 with 3 requests\n",
    "\n",
    "print(\"\\nRequest 1: Show me movies similar to 'Toy Story' using Heuristic 2\")\n",
    "heuristic_2_results_toy_story = combined_heuristic_2(metadataDF, \"Toy Story\")\n",
    "heuristic_2_df_toy_story = display_heuristic_results(heuristic_2_results_toy_story, \"Toy Story\", top_n=10)\n",
    "\n",
    "print(\"\\nRequest 2: Show me movies similar to 'Apollo 13' using Heuristic 2\")\n",
    "heuristic_2_results_apollo_13 = combined_heuristic_2(metadataDF, \"Apollo 13\")\n",
    "heuristic_2_df_apollo_13 = display_heuristic_results(heuristic_2_results_apollo_13, \"Apollo 13\", top_n=10)\n",
    "\n",
    "print(\"\\nRequest 3: Show me movies similar to 'Fight Club' using Heuristic 2\")\n",
    "heuristic_2_results_fight_club = combined_heuristic_2(metadataDF, \"Fight Club\")\n",
    "heuristic_2_df_fight_club = display_heuristic_results(heuristic_2_results_fight_club, \"Fight Club\", top_n=10)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "#### Heuristic #1 \n",
    "Based on my personal knowledge of the movies, the combination of genre and popularity generated movies that were quite similar to the ones in the queries. Something I noticed was interesting though was that when we queried `Request 2: Show me movies similar to 'Apollo 13' using Heuristic 1`, Fight Club was the second result. But when we did the reverse `Request 3: Show me movies similar to 'Fight Club' using Heuristic 1`, Apollo 13 was not listed as a similar movie. \n",
    "\n",
    "#### Heuristic #2 \n",
    "In contrast to heuristic #1, I dont believe that the combination of Movie Title and Budget was an effective way to find similar movies. From personal experience, movies can have very similar titles yet filmed for completely different audiences. This is similar for budget where the price of a movie isnt an effective way to tell if movies are similar. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the utility matrix (Rui)\n",
    "utility_matrix = ratingsSmallDF.pivot_table(index='userId', columns='movieId', values='rating')\n",
    "\n",
    "# Fill NaN values with 0 (indicating unrated movies)\n",
    "utility_matrix = utility_matrix.fillna(0)\n",
    "\n",
    "# Convert utility matrix to numpy array for matrix factorization\n",
    "R = utility_matrix.values\n",
    "\n",
    "def matrix_factorization(R, K, steps=5000, alpha=0.0002, beta=0.02):\n",
    "    '''\n",
    "    R: rating matrix\n",
    "    P: |U| * K (User features matrix)\n",
    "    Q: |D| * K (Item features matrix)\n",
    "    K: latent features\n",
    "    steps: iterations\n",
    "    alpha: learning rate\n",
    "    beta: regularization parameter'''\n",
    "\n",
    "    # Initialize P and Q randomly\n",
    "    num_users, num_items = R.shape\n",
    "    P = np.random.rand(num_users, K)\n",
    "    Q = np.random.rand(num_items, K)\n",
    "    Q = Q.T  # Transpose Q to match matrix multiplication\n",
    "\n",
    "    # Perform SGD to factorize R\n",
    "    for step in range(steps):\n",
    "        for i in range(num_users):\n",
    "            for j in range(num_items):\n",
    "                if R[i][j] > 0:  # Only update for rated movies\n",
    "                    eij = R[i][j] - np.dot(P[i, :], Q[:, j])  # Calculate error\n",
    "\n",
    "                    for k in range(K):  # Update the P and Q matrices\n",
    "                        P[i][k] += alpha * (2 * eij * Q[k][j] - beta * P[i][k])\n",
    "                        Q[k][j] += alpha * (2 * eij * P[i][k] - beta * Q[k][j])\n",
    "\n",
    "        # Calculate the error and regularization term\n",
    "        e = 0\n",
    "        for i in range(num_users):\n",
    "            for j in range(num_items):\n",
    "                if R[i][j] > 0:\n",
    "                    e += (R[i][j] - np.dot(P[i, :], Q[:, j]))**2\n",
    "                    for k in range(K):\n",
    "                        e += (beta / 2) * (P[i][k]**2 + Q[k][j]**2)\n",
    "\n",
    "        # Stop if the error is below a threshold\n",
    "        if e < 0.001:\n",
    "            break\n",
    "\n",
    "    return P, Q.T  # Return the factorized matrices\n",
    "\n",
    "# Perform matrix factorization with 5 latent factors (K=5)\n",
    "K = 5\n",
    "P, Q = matrix_factorization(R, K)\n",
    "\n",
    "# Check the shape of P and Q\n",
    "print(\"P shape:\", P.shape)\n",
    "print(\"Q shape:\", Q.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "In this notebook, we explored different similarity measures, clustering techniques, combined heuristics, and reccomender systems. Unfortunately, we were unable to finish study #4 becuase of time constraints but as next steps, we would finish the implementation.\n",
    "\n",
    "### References:\n",
    "- Clustering Algorithm KBmeans: https://www.w3schools.com/python/python_ml_k-means.asp\n",
    "- Matrix Factorization: https://medium.com/data-science/recommendation-system-matrix-factorization-d61978660b4b"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
